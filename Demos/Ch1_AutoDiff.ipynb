{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cu-applied-math/appm-4600-numerics/blob/main/Demos/Ch1_AutoDiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Differentiation demo\n",
        "Using Jax and PyTorch\n",
        "\n",
        "APPM 4600\n",
        "\n",
        "Copyright Dept of Applied Math, University of Colorado Boulder. Released under a BSD 3-clause license\n",
        "\n",
        "Learning objectives:\n",
        "1. See how to use AutoDiff using two popular frameworks (jax and PyTorch)\n",
        "2. See that reverse mode is usually faster (than forward mode) for functions $f:\\mathbb{R}^n \\to \\mathbb{R}$\n",
        "3. Compare to symbolic differentiation\n",
        "\n",
        "Further reading\n",
        "- another [AutoDiff](https://github.com/cu-applied-math/SciML-Class/blob/main/Demos/AutomaticDifferentiation.ipynb) demo from CU\n",
        "- [JAX](https://docs.jax.dev/en/latest/index.html)\n",
        "- [PyTorch](https://pytorch.org/)"
      ],
      "metadata": {
        "id": "GplvV8syNQtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using jax"
      ],
      "metadata": {
        "id": "eIlDxOqfNZBO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQiN5MqMMJDP"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import jacfwd, jacrev\n",
        "from jax import nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll make a simple function. Note that the \"@\" sign is matrix multiplication (for either jax or numpy), i.e., [jax.numpy.matmul](https://docs.jax.dev/en/latest/_autosummary/jax.numpy.matmul.html#jax.numpy.matmul)"
      ],
      "metadata": {
        "id": "Yl89jUE8RB0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(1e1)\n",
        "m = int(n/2)\n",
        "\n",
        "# We want some arbitrary matrix -- e.g., we could do this randomly\n",
        "# jax has some utilities for this, but if you don't want to learn them,\n",
        "# just convert from numpy\n",
        "A = np.random.randn(m,n)\n",
        "x = np.random.randn(n,1)\n",
        "A = jnp.array(A)\n",
        "x = jnp.array(x)\n",
        "\n",
        "def f(x):\n",
        "    return jnp.sum( A @ x )"
      ],
      "metadata": {
        "id": "1RanQp4iMSU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can ask jax for:\n",
        "- the gradient (of a function $f: \\mathbb{R}^n \\to \\mathbb{R}$)\n",
        "- the Jacobian (of a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$)\n",
        "  - if $m=1$ this *is* the gradient! (though sometimes there is a transpose difference...)\n",
        "\n",
        "Gradients are always computed via **reverse mode**, but for Jacobians, you can choose either **reverse** or **forward** mode. In general, if $n > m$ you want **reverse** mode. See Jax's [\"Autodiff Cookbook\"](https://docs.jax.dev/en/latest/notebooks/autodiff_cookbook.html)"
      ],
      "metadata": {
        "id": "q_Tok6FoNhLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g  = grad(f)\n",
        "J1 = jacfwd(f)\n",
        "J2 = jacrev(f)\n",
        "\n",
        "g(x), J1(x), J2(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FqkGJkWNBsb",
        "outputId": "105f40cf-772d-41de-8793-d6c1f8d6c79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([[ 0.10704881],\n",
              "        [ 4.605774  ],\n",
              "        [ 1.5944297 ],\n",
              "        [ 1.575459  ],\n",
              "        [ 0.11993957],\n",
              "        [-3.6401691 ],\n",
              "        [-1.0900795 ],\n",
              "        [ 1.6887466 ],\n",
              "        [-2.1338854 ],\n",
              "        [-6.1179714 ]], dtype=float32),\n",
              " Array([[ 0.10704881],\n",
              "        [ 4.605774  ],\n",
              "        [ 1.5944297 ],\n",
              "        [ 1.575459  ],\n",
              "        [ 0.11993957],\n",
              "        [-3.6401691 ],\n",
              "        [-1.0900795 ],\n",
              "        [ 1.6887466 ],\n",
              "        [-2.1338854 ],\n",
              "        [-6.1179714 ]], dtype=float32),\n",
              " Array([[ 0.10704881],\n",
              "        [ 4.605774  ],\n",
              "        [ 1.5944297 ],\n",
              "        [ 1.575459  ],\n",
              "        [ 0.11993957],\n",
              "        [-3.6401691 ],\n",
              "        [-1.0900795 ],\n",
              "        [ 1.6887466 ],\n",
              "        [-2.1338854 ],\n",
              "        [-6.1179714 ]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's be slightly more interesting"
      ],
      "metadata": {
        "id": "5XC7c41SaS3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(5e3)\n",
        "m = n\n",
        "k = n\n",
        "\n",
        "key = jax.random.key(seed=0)\n",
        "A = jax.random.normal(key, (m,n))\n",
        "B = jax.random.normal(key, (k,m))\n",
        "x = jax.random.normal(key, (n,1))\n",
        "# A = jnp.array(np.random.randn(m,n)) # another (slower) way to do it\n",
        "# B = jnp.array(np.random.randn(k,m))\n",
        "# x = jnp.array(np.random.randn(n,1))\n",
        "\n",
        "def f(x):\n",
        "    return jnp.sum( nn.sigmoid(B @ nn.sigmoid(A @ x ) ) )\n",
        "\n",
        "# The first time we call the function, it is doing some overhead\n",
        "%time y = f(x)"
      ],
      "metadata": {
        "id": "YcVxHHiHNWMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8a89fd-692b-480a-8dfd-8ff1b2bf2a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 462 ms, sys: 15.1 ms, total: 477 ms\n",
            "Wall time: 239 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time y = f(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9yFNCfvQnpW",
        "outputId": "6a94408e-d3d8-49f4-afeb-8d661e63d79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.24 ms, sys: 23 Âµs, total: 6.26 ms\n",
            "Wall time: 4.95 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g  = grad(f)\n",
        "J1 = jacfwd(f)\n",
        "J2 = jacrev(f)"
      ],
      "metadata": {
        "id": "-U611acvPRcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "y = g(x)  # reverse-mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nyz6jkqfOHEW",
        "outputId": "d9feb3d4-72ba-45a8-cfc4-d8d14b1f763c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 304 ms, sys: 8.11 ms, total: 312 ms\n",
            "Wall time: 234 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "y = J1(x) # forward-mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr2osPaOOKVJ",
        "outputId": "a3387ade-2dc6-4ad0-e8da-f76df55b895d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.16 s, sys: 758 ms, total: 1.92 s\n",
            "Wall time: 1.08 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "y = J2(x) # reverse-mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkMddiRVPTXY",
        "outputId": "82a5b0a9-816b-41dc-c634-66bb9262efe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 554 ms, sys: 85.1 ms, total: 639 ms\n",
            "Wall time: 342 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We seee that reverse-mode (`J2` and `g`) are faster than forward mode (`J1`). Now, naively you'd expect them to be **way** faster, but I think jax is being somewhat clever about how it does the forward mode"
      ],
      "metadata": {
        "id": "JHK4zHn3S3Tk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's repeat the same thing in PyTorch\n",
        "PyTorch is another popular autodiff framework"
      ],
      "metadata": {
        "id": "XZXtWrR_qKas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "from torch.nn.functional import sigmoid\n",
        "print(\"Torch version is\", torch.__version__)\n",
        "print(\"Numpy version is\", np.__version__)\n",
        "print(\"Python version is\", sys.version)\n",
        "\n",
        "torch.manual_seed(100)\n",
        "# dtype = torch.float32 # the default\n",
        "dtype = torch.float64\n",
        "\n",
        "n = int(8e3)\n",
        "m = n\n",
        "k = n\n",
        "\n",
        "A = torch.randn((m,n),dtype=dtype)\n",
        "B = torch.randn((k,m),dtype=dtype)\n",
        "x = torch.randn((n,1), dtype=dtype, requires_grad=True)\n",
        "\n",
        "def f(x):\n",
        "    return torch.sum( sigmoid(B @ sigmoid(A @ x ) ) )\n",
        "\n",
        "y = f(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkXQzUcDRYnM",
        "outputId": "2139c3a1-4ce1-494f-cdb1-d61f7f49acfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version is 2.8.0+cu126\n",
            "Numpy version is 2.0.2\n",
            "Python version is 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "if x.grad is not None:\n",
        "    x.grad.data.zero_()\n",
        "out = f(x)\n",
        "out.backward()\n",
        "y = x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll6tXP598HTY",
        "outputId": "c5ef1e5e-8d91-4550-e8b9-adaf12330856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 291 ms, sys: 1.26 ms, total: 292 ms\n",
            "Wall time: 314 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speelpenning Function\n",
        "Taken from the longer [SciML AutoDiff](https://github.com/cu-applied-math/SciML-Class/blob/main/Demos/AutomaticDifferentiation.ipynb) example"
      ],
      "metadata": {
        "id": "YI3x9niGTrRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy\n",
        "from sympy.abc import x\n",
        "roots = np.linspace(0,1,10)\n",
        "def g(x):\n",
        "    y = 1\n",
        "    for i in range(len(roots)):\n",
        "        y = y * (x - roots[i])\n",
        "    return y\n",
        "g(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        },
        "id": "cukxRIAiTvwR",
        "outputId": "8987a2e1-4e2f-40a5-e0a9-e96a3915fd9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111)"
            ],
            "text/latex": "$\\displaystyle x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right)$"
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gprime = sympy.diff(g(x),x)\n",
        "gprime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "1tFe7aVQT8th",
        "outputId": "ac888b1d-b6a3-4705-ef36-6bf08fe665e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222) + x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.111111111111111) + x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.222222222222222)*(x - 0.111111111111111) + x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111) + x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111) + x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111) + x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111) + x*(x - 1.0)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111) + x*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111) + (x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111)"
            ],
            "text/latex": "$\\displaystyle x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) + x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 1.0\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right)$"
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gprime.evalf(16,subs={x:.88889})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "AHNTIuBVUWOt",
        "outputId": "07c6c712-9e36-4b90-fff0-dda57258f594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0001040765432583041"
            ],
            "text/latex": "$\\displaystyle -0.0001040765432583041$"
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That symbolic deriviative is **correct**, but it's not an efficient implementation. We can get an efficient implementation if we play around a bit, but it's not automatic.\n",
        "\n",
        "For example, we can tell sympy to expand $g(x)$ out, and *then* differentiate:"
      ],
      "metadata": {
        "id": "Q80Dk__SVUno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sympy.expand(g(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "id": "MSsVDLGEVGPq",
        "outputId": "1d7c7780-d3c1-4f7d-f638-0f45b4407545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x**10 - 5.0*x**9 + 10.7407407407407*x**8 - 12.962962962963*x**7 + 9.64380429812528*x**6 - 4.56104252400549*x**5 + 1.36173159391165*x**4 - 0.245182437937607*x**3 + 0.0238479488367999*x**2 - 0.000936656708416885*x"
            ],
            "text/latex": "$\\displaystyle x^{10} - 5.0 x^{9} + 10.7407407407407 x^{8} - 12.962962962963 x^{7} + 9.64380429812528 x^{6} - 4.56104252400549 x^{5} + 1.36173159391165 x^{4} - 0.245182437937607 x^{3} + 0.0238479488367999 x^{2} - 0.000936656708416885 x$"
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sympy.diff( sympy.expand(g(x)), x )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "id": "Um7mr_t3VgFr",
        "outputId": "79f492de-edf5-45be-f1fc-cce5bb14d10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10*x**9 - 45.0*x**8 + 85.9259259259259*x**7 - 90.7407407407407*x**6 + 57.8628257887517*x**5 - 22.8052126200274*x**4 + 5.44692637564659*x**3 - 0.735547313812822*x**2 + 0.0476958976735998*x - 0.000936656708416885"
            ],
            "text/latex": "$\\displaystyle 10 x^{9} - 45.0 x^{8} + 85.9259259259259 x^{7} - 90.7407407407407 x^{6} + 57.8628257887517 x^{5} - 22.8052126200274 x^{4} + 5.44692637564659 x^{3} - 0.735547313812822 x^{2} + 0.0476958976735998 x - 0.000936656708416885$"
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Showing that AutoDiff depends on the implementation\n",
        "\n",
        "We'll define the function $f(x)=0$ but in a slow way"
      ],
      "metadata": {
        "id": "pzbhKuC0e_D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "d   = int(4e3)\n",
        "\n",
        "torch.manual_seed(100)\n",
        "A   = torch.randn( (d,d) )\n",
        "\n",
        "def f(x, N = 100):\n",
        "    \"\"\" Implements the zero function: f(x) = 0 \"\"\"\n",
        "    for k in range(N):\n",
        "        x = A @ x\n",
        "\n",
        "    return torch.sum(x - x)\n",
        "\n",
        "x   = torch.randn( (d,1), requires_grad=True )"
      ],
      "metadata": {
        "id": "rf0F75eDVpsJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with torch.no_grad():\n",
        "    y = f(x)"
      ],
      "metadata": {
        "id": "iPH7qopFfKBj",
        "outputId": "4b79ddf9-fde8-4220-a5cc-68a1848de255",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 564 ms, sys: 3 Âµs, total: 564 ms\n",
            "Wall time: 567 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradient is the all zeros vector, but as you can see from the time it takes to execute the code, it's not being that clever..."
      ],
      "metadata": {
        "id": "qYe55YIFfi9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "y = f(x)\n",
        "y.backward()"
      ],
      "metadata": {
        "id": "ZbJiRbGSfSMq",
        "outputId": "46110da4-604e-4aa2-c9e8-721708a2b244",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.25 s, sys: 5.3 ms, total: 1.26 s\n",
            "Wall time: 1.32 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JnOs0YUBffQ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}